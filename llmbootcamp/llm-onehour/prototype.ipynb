{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick mockup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "open_ai_key = getpass.getpass(\"Open AI Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = open_ai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "\n",
    "# Create an LLM object\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nHello there! It's nice to meet you. What kind of test are you referring to?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Hello, this is a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Hello there! It's nice to meet you. What kind of test are you referring to?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(llm(\"Hello, this is a test.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Zero-shot chain-of-thought prompting is a technique used to help people with cognitive impairments to generate ideas and thoughts. It involves prompting the person with a series of questions that are related to the topic at hand. The questions are designed to help the person to think of related ideas and to build a chain of thought. This technique can be used to help people with memory problems, language impairments, and other cognitive impairments to generate ideas and thoughts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm(\"What is zero-shot chain-of-thought prompting?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's bring in some context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pretrained large language models (LLMs) are widely used in many sub-fields of\n",
       "natural language processing (NLP) and generally known as excellent few-shot\n",
       "learners with task-specific exemplars. Notably, chain of thought (CoT)\n",
       "prompting, a recent technique for eliciting complex multi-step reasoning\n",
       "through step-by-step answer examples, achieved the state-of-the-art\n",
       "performances in arithmetics and symbolic reasoning, difficult system-2 tasks\n",
       "that do not follow the standard scaling laws for LLMs. While these successes\n",
       "are often attributed to LLMs' ability for few-shot learning, we show that LLMs\n",
       "are decent zero-shot reasoners by simply adding \"Let's think step by step\"\n",
       "before each answer. Experimental results demonstrate that our Zero-shot-CoT,\n",
       "using the same single prompt template, significantly outperforms zero-shot LLM\n",
       "performances on diverse benchmark reasoning tasks including arithmetics\n",
       "(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\n",
       "Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\n",
       "Objects), without any hand-crafted few-shot examples, e.g. increasing the\n",
       "accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\n",
       "large InstructGPT model (text-davinci-002), as well as similar magnitudes of\n",
       "improvements with another off-the-shelf large model, 540B parameter PaLM. The\n",
       "versatility of this single prompt across very diverse reasoning tasks hints at\n",
       "untapped and understudied fundamental zero-shot capabilities of LLMs,\n",
       "suggesting high-level, multi-task broad cognitive capabilities may be extracted\n",
       "by simple prompting. We hope our work not only serves as the minimal strongest\n",
       "zero-shot baseline for the challenging reasoning benchmarks, but also\n",
       "highlights the importance of carefully exploring and analyzing the enormous\n",
       "zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\n",
       "few-shot exemplars."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "paper = next(arxiv.Search(id_list=[\"2205.11916\"]).results()) #The next function returns the next item in an iterator\n",
    "\n",
    "Markdown(paper.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Zero-shot chain-of-thought prompting is a technique for eliciting complex multi-step reasoning through step-by-step answer examples without the need for hand-crafted few-shot examples. This technique uses a single prompt template that is versatile across very diverse reasoning tasks, and has been shown to significantly outperform zero-shot language model performances on benchmark reasoning tasks. This suggests that high-level, multi-task broad cognitive capabilities may be extracted by simple prompting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(f\"\"\"Here's a summary of the paper: {paper.summary})\n",
    "\n",
    "Based on that summary, what is zero-shot chain-of-thought prompting?\"\"\")\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the paper searchable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_path = paper.download_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(paper_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\\n\\n\".join([page.page_content for page in pages[0:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Zero-shot chain-of-thought prompting is a technique for eliciting complex multi-step reasoning through step-by-step answer examples without the need for any hand-crafted few-shot examples. It involves adding a prompt, such as \"Let's think step by step,\" before each answer to facilitate step-by-step thinking. This technique has been shown to significantly outperform zero-shot language model performances on diverse benchmark reasoning tasks, including arithmetics, symbolic reasoning, and other logical reasoning tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(f\"\"\"Here's the first two pages of a paper: {content})\n",
    "\n",
    "Based on that content, what is zero-shot chain-of-thought prompting?\"\"\")\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find relevant content using embedding search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(\"What is zero-shot chain-of-thought prompting?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "language models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\n",
       "few-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\n",
       "given for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\n",
       "in the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\n",
       "asFew-shot-CoT in this work.\n",
       "3 Zero-shot Chain of Thought\n",
       "We propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\n",
       "It differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\n",
       "step-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\n",
       "2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\n",
       "with a single template. The core idea of our method is simple, as described in Figure 1: add Let’s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(docs[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an existing chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "query = \"What is zero-shot chain-of-thought prompting?\"\n",
    "sources = db.similarity_search(query)\n",
    "results = chain({\"input_documents\": sources, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Zero-shot chain-of-thought prompting is a single zero-shot prompt that elicits chain of thought from large language models across a variety of reasoning tasks, without requiring hand-crafting few-shot examples per task.\n",
       "SOURCES: ./2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(results[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
