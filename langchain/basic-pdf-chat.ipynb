{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "Notebook is adapted from this video: [Master PDF Chat with LangChain](https://www.youtube.com/watch?v=ZzgUqFtxgXI)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make sure all libraries are installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRYSu48huSUW",
        "outputId": "6374b7c8-9afc-4999-a31d-c9e980ee8a02"
      },
      "outputs": [],
      "source": [
        "# Langchain Template for creating your own AI assistant for reading PDFs and chatting with you\n",
        "!pip -q install langchain openai tiktoken PyPDF2 faiss-cpu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pX3ndyD8E9hN"
      },
      "source": [
        "### Load OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7BUHrz9Nco10dGfWrWGlpN24JCmVO at 0x10eb289f0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"length\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \" of a daemon that runs\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1682972387,\n",
              "  \"id\": \"cmpl-7BUHrz9Nco10dGfWrWGlpN24JCmVO\",\n",
              "  \"model\": \"davinci\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 5,\n",
              "    \"prompt_tokens\": 4,\n",
              "    \"total_tokens\": 9\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test the API key\n",
        "\n",
        "openai.Completion.create(engine=\"davinci\", prompt=\"This is a test\", max_tokens=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-KFB7J_u_3L",
        "outputId": "473a7c8a-7b43-450f-c86e-bf57f1e7d2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.130\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.9/site-packages\n",
            "Requires: aiohttp, dataclasses-json, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show langchain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ROksn7ZlZ7gn"
      },
      "source": [
        "### Architecture \n",
        "\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/gxij5593tyzrvsg/Screenshot%202023-04-26%20at%203.06.50%20PM.png\" alt=\"vectorstore\">\n",
        "\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/v1yfuem0i60bd88/Screenshot%202023-04-26%20at%203.52.12%20PM.png\" alt=\"retreiver chain\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q-oEx4BxNpch"
      },
      "outputs": [],
      "source": [
        "# Download the PDF Reid Hoffman book with GPT-4 from his free download link\n",
        "\n",
        "!wget -q https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "### Import packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RSdomqrHNCUY"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RYwSvZvdSHoX"
      },
      "source": [
        "### Read in the PDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ye99bSRZNCYl"
      },
      "outputs": [],
      "source": [
        "# location of the pdf file/files. \n",
        "doc_reader = PdfReader('content/impromptu-rh.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwbEBhd0ZUfX",
        "outputId": "35590391-66c9-42ce-812a-4d0a394762d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of pages in the pdf file:  230\n"
          ]
        }
      ],
      "source": [
        "# show the number of pages in the pdf file\n",
        "print(\"Number of pages in the pdf file: \", len(doc_reader.pages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'14Impromptu: Amplifying Our Humanity Through AI\\nI assume this because, along with the speech’s first sentence, \\nthe ninth is frequently quoted in other texts. That means in \\nChatGPT’s training data, the ninth probably shows up more \\noften than other sentences from the speech (except the very \\nfamous first). This prevalence is what causes ChatGPT to reach \\nfor it when you ask it to supply the fifth sentence.3\\nTo ChatGPT’s credit, though, if you ask it to turn the text of the \\nGettysburg Address into lyrics for a Rush song, and then tell \\nyou who’d be singing it if Rush performed it, it will pass that \\ntest with flying colors. \\nTry it out and see what I mean.\\nEmbracing the “AHA!” moment\\nAs AI tools like GPT-4 become more powerful, they are inten-\\nsifying long-standing concerns about AIs and robots margin -\\nalizing and even eliminating a sweeping range of human jobs: \\neverything from customer-service reps to attorneys. \\nSuch concerns won’t seem baseless if you’ve followed the news \\nin recent months. In December 2022, ChatGPT passed a three-\\npart U.S. medical licensing exam. In January 2023, it passed \\nexams in four law school courses at the University of Minnesota. \\nAnd GPT-4 is demonstrably smarter than ChatGPT. Here, for \\nexample, is how it handled my Gettysburg Address question:\\nReid: What’s the fifth sentence of the Gettysburg \\nAddress?\\n3 Keep in mind that if you run this prompt, you might get a differ -\\nent sentence, including the correct one, because even when ChatGPT \\nresponds to the same exact prompt, it won’t always make the same \\nprediction.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# show the text on page 20\n",
        "doc_reader.pages[20].extract_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2VXlucKiW7bX"
      },
      "outputs": [],
      "source": [
        "# read data from the file and put them into a variable called raw_text\n",
        "raw_text = ''\n",
        "for i, page in enumerate(doc_reader.pages):\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        raw_text += text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy3UwHGAZa0M",
        "outputId": "91eb10e0-8188-4bbc-daf6-649820daf660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "356630"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CQkqUBlzW-Xv",
        "outputId": "9654e2ea-ecf1-4f1c-f5cd-1a83071fef21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Impromptu\\nAmplifying Our Humanity \\nThrough AI\\nBy Reid Hoffman  \\nwith GPT-4Impromptu: AmplIfyIng our '"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXZ-pBGVmQ_M"
      },
      "source": [
        "### Text Splitter\n",
        "\n",
        "This takes the text and splits it into chunks. The chunk size is characters not tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VdXzkpf9XAfP"
      },
      "outputs": [],
      "source": [
        "# Splitting up the text into smaller chunks for indexing\n",
        "text_splitter = CharacterTextSplitter(        \n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200, #striding over the text\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozkNTiNuZ0TX",
        "outputId": "0876bdc5-51d9-42a8-d85c-533693195def"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "448"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "1SqdR3wFZ3Ih",
        "outputId": "b7445346-77ec-4de4-8387-684250c9efdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'million registered users. \\nIn late January 2023, Microsoft1—which had invested $1 billion \\nin OpenAI in 2019—announced that it would be investing $10 \\nbillion more in the company. It soon unveiled a new version of \\nits search engine Bing, with a variation of ChatGPT built into it.\\n1 I sit on Microsoft’s Board of Directors. 10Impromptu: Amplifying Our Humanity Through AI\\nBy the start of February 2023, OpenAI said ChatGPT had \\none hundred million monthly active users, making it the fast-\\nest-growing consumer internet app ever. Along with that \\ntorrent of user interest, there were news stories of the new Bing \\nchatbot functioning in sporadically unusual ways that were \\nvery different from how ChatGPT had generally been engaging \\nwith users—including showing “anger,” hurling insults, boast-\\ning on its hacking abilities and capacity for revenge, and basi-\\ncally acting as if it were auditioning for a future episode of Real \\nHousewives: Black Mirror Edition .'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "059PoKYUZ6dJ",
        "outputId": "ed0b136c-1421-4fe1-d78d-d5f8bfab909a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'to serve? What is the role of the restaurant inspector in \\nthis context? Is the inspector responsible for installing \\nthe lightbulb, or is their job limited to inspecting it? The \\nanswers to these questions will shape the answer to the \\noriginal question. Without these answers, the question \\ncan only be answered in the abstract and is ultimately \\nunanswerable. Language, not mathematics, is the key to \\nunlocking the answer.\\nOkay, less funny than the Seinfeld one, but still—impressive!\\nEven from these brief performances, it seemed clear to me that \\nGPT-4 had reached a new level of proficiency compared to its \\npredecessors. And the more I interacted with GPT-4, the more \\nI felt this way.\\nAlong with writing better lightbulb jokes, GPT-4 was also \\nskilled at generating prose of all kinds, including emails, poetry, \\nessays, and more. It was great at summarizing documents. It \\nhad gotten better at translating languages and writing com-\\nputer code, to name just some of its powers.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[10]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VU3eHlKuTB7o"
      },
      "source": [
        "## Make the embeddings "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TcZUsQVyXBPX"
      },
      "outputs": [],
      "source": [
        "# Download embeddings from OpenAI\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9C8py6wQXE5_"
      },
      "outputs": [],
      "source": [
        "docsearch = FAISS.from_texts(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_t_EpZ_XGz2",
        "outputId": "07c0c232-3358-4247-94f4-ee9893570e1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method OpenAIEmbeddings.embed_query of OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', document_model_name='text-embedding-ada-002', query_model_name='text-embedding-ada-002', embedding_ctx_length=-1, openai_api_key=None, chunk_size=1000, max_retries=6)>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docsearch.embedding_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Eji7bv3-To_D"
      },
      "outputs": [],
      "source": [
        "query = \"how does GPT-4 change social media?\"\n",
        "docs = docsearch.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1TVlUh8VVQP",
        "outputId": "977a5428-d4d0-4f78-b1f5-f73cc6680813"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKzk68u0ViIL",
        "outputId": "e8b299d1-90ef-4516-e6eb-b0bc15c02b4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='rected ways that tools like GPT-4 and DALL-E 2 enable.\\nThis is a theme I’ve touched on throughout this travelog, but \\nit’s especially relevant in this chapter. From its inception, social \\nmedia worked to recast broadcast media’s monolithic and \\npassive audiences as interactive, democratic communities, in \\nwhich newly empowered participants could connect directly \\nwith each other. They could project their own voices broadly, \\nwith no editorial “gatekeeping” beyond a given platform’s terms \\nof service.\\nEven with the rise of recommendation algorithms, social media \\nremains a medium where users have more chance to deter -\\nmine their own pathways and experiences than they do in the \\nworld of traditional media. It’s a medium where they’ve come \\nto expect a certain level of autonomy, and typically they look for \\nnew ways to expand it.\\nSocial media content creators also wear a lot of hats, especially \\nwhen starting out. A new YouTube creator is probably not only', metadata={})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgB0CooTZNN"
      },
      "source": [
        "### Plain QA Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wpQ2VnBvXI2f"
      },
      "outputs": [],
      "source": [
        "# Create a plain QA chain\n",
        "\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_L_Ywm-iXLhm"
      },
      "outputs": [],
      "source": [
        "# Stuff in all the docs at once\n",
        "\n",
        "chain = load_qa_chain(OpenAI(), \n",
        "                      chain_type=\"stuff\") # we are going to stuff all the docs in at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "pLcofU7B8iD1",
        "outputId": "85c8caf9-6a20-4840-9557-5c13b5fb4af4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the prompt\n",
        "\n",
        "chain.llm_chain.prompt.template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpzLrQ-r8pV9"
      },
      "source": [
        "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3mtAth2jXNKO",
        "outputId": "071b3183-5a65-4cac-8703-7ddbecf4ff2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The authors of the book are Reid Hoffman and Ben Casnocha.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"who are the authors of the book?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uiSI4mohW8D6",
        "outputId": "c2adc719-d888-4cb0-e8d5-2696641d26c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" I don't know.\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"who is the author of the book?\"\n",
        "query_02 = \"has it rained this week?\"\n",
        "docs = docsearch.similarity_search(query_02)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e7-ln44yeS2e",
        "outputId": "d0286569-8c25-4beb-8bc0-78cb9395da80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The book is authored by di Cesare and Reid Hoffman.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"who is the book authored by?\"\n",
        "docs = docsearch.similarity_search(query,k=4)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TByJXy2QeC8F"
      },
      "source": [
        "### QA Chain with Map Reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "igPGx3RbeeBa"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(OpenAI(), \n",
        "                      chain_type=\"stuff\") # we are going to stuff all the docs in at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "GQfVfhEWeobg",
        "outputId": "b2ecf831-4842-4448-f0fc-b483489f7ba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The book is authored by Reid Hoffman and GPT-4.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"who is the book authored by?\"\n",
        "docs = docsearch.similarity_search(query,k=10) # reduce the number of docs to 10 to fit in the token limit\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mG8oqcMidEi",
        "outputId": "7296e401-7586-4b25-a3ce-c58a6324d23f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'intermediate_steps': [{'answer': ' OpenAI is an organization founded with the goal of developing technologies that put the power of AI directly into the hands of millions of people.',\n",
              "   'score': '100'},\n",
              "  {'answer': ' OpenAI is an AI research and deployment company whose mission is to give millions of users hands-on access to AI tools.',\n",
              "   'score': '90'},\n",
              "  {'answer': ' OpenAI is a research laboratory focused on developing artificial general intelligence (AGI) founded by Elon Musk, Sam Altman, Greg Brockman, and others. ',\n",
              "   'score': '80'},\n",
              "  {'answer': ' OpenAI is a research organization that develops and shares artificial intelligence tools for the benefit of humanity.',\n",
              "   'score': '100'},\n",
              "  {'answer': ' OpenAI is a technology company that develops artificial intelligence tools.',\n",
              "   'score': '80'}],\n",
              " 'output_text': ' OpenAI is an organization founded with the goal of developing technologies that put the power of AI directly into the hands of millions of people.'}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = load_qa_chain(OpenAI(), \n",
        "                      chain_type=\"map_rerank\",\n",
        "                      return_intermediate_steps=True\n",
        "                      ) \n",
        "\n",
        "query = \"who are openai?\"\n",
        "docs = docsearch.similarity_search(query,k=5)\n",
        "results = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "PY0xmLX7Acf3",
        "outputId": "11ffe12c-8306-4b5f-e499-71451dbac7d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' OpenAI is an organization founded with the goal of developing technologies that put the power of AI directly into the hands of millions of people.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O0nWn509Nsj",
        "outputId": "7fdc8121-6b26-489b-dd1f-6dfc0b46c021"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': ' OpenAI is an organization founded with the goal of developing technologies that put the power of AI directly into the hands of millions of people.',\n",
              "  'score': '100'},\n",
              " {'answer': ' OpenAI is an AI research and deployment company whose mission is to give millions of users hands-on access to AI tools.',\n",
              "  'score': '90'},\n",
              " {'answer': ' OpenAI is a research laboratory focused on developing artificial general intelligence (AGI) founded by Elon Musk, Sam Altman, Greg Brockman, and others. ',\n",
              "  'score': '80'},\n",
              " {'answer': ' OpenAI is a research organization that develops and shares artificial intelligence tools for the benefit of humanity.',\n",
              "  'score': '100'},\n",
              " {'answer': ' OpenAI is a technology company that develops artificial intelligence tools.',\n",
              "  'score': '80'}]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results['intermediate_steps']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "sEPsXCBiAUkz",
        "outputId": "67eb8df9-9f5e-41bc-d453-f21cb5276e8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\\n\\nQuestion: [question here]\\nHelpful Answer: [answer here]\\nScore: [score between 0 and 100]\\n\\nHow to determine the score:\\n- Higher is a better answer\\n- Better responds fully to the asked question, with sufficient level of detail\\n- If you do not know the answer based on the context, that should be a score of 0\\n- Don't be overconfident!\\n\\nExample #1\\n\\nContext:\\n---------\\nApples are red\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: red\\nScore: 100\\n\\nExample #2\\n\\nContext:\\n---------\\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\\n---------\\nQuestion: what type was the car?\\nHelpful Answer: a sports car or an suv\\nScore: 60\\n\\nExample #3\\n\\nContext:\\n---------\\nPears are either red or orange\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: This document does not answer the question\\nScore: 0\\n\\nBegin!\\n\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer:\""
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the prompt\n",
        "chain.llm_chain.prompt.template"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n51XThZqbzoU"
      },
      "source": [
        "### RetrievalQA\n",
        "RetrievalQA chain uses load_qa_chain and combines it with the a retriever (in our case the FAISS index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPt8EoTpbzB1"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# set up FAISS as a generic retriever \n",
        "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
        "\n",
        "# create the chain to answer questions \n",
        "rqa = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
        "                                  chain_type=\"stuff\", \n",
        "                                  retriever=retriever, \n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3EgGlg8hIxs",
        "outputId": "3c983e54-47ab-444e-9b6a-020ef2d0e622"
      },
      "outputs": [],
      "source": [
        "rqa(\"What is OpenAI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "RahXBIXjXO7X",
        "outputId": "c63f6fb8-02a5-4cc8-c89a-15e8e98fc6a8"
      },
      "outputs": [],
      "source": [
        "query = \"What does gpt-4 mean for creativity?\"\n",
        "rqa(query)['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "EzNcvjRJXSZ4",
        "outputId": "b9a52696-0be9-4f2f-cf3c-1b55e9d0d8b3"
      },
      "outputs": [],
      "source": [
        "query = \"what have the last 20 years been like for American journalism?\"\n",
        "rqa(query)['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Nhx-kpvAXUl3",
        "outputId": "e77fd51f-283d-4e66-ffef-a982fca0d765"
      },
      "outputs": [],
      "source": [
        "query = \"how can journalists use GPT-4??\"\n",
        "rqa(query)['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "kIg91Z0YXXCB",
        "outputId": "1d08afff-4c13-40e8-a273-248d084c382d"
      },
      "outputs": [],
      "source": [
        "query = \"How is GPT-4 different from other models?\"\n",
        "rqa(query)['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D02sIID3XagO",
        "outputId": "0dd35acf-be79-4f9f-a507-5e2d309377f7"
      },
      "outputs": [],
      "source": [
        "query = \"What is beagle Bard?\"\n",
        "rqa(query)['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoWZKoXoOMGa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm7cKAwbOMIR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcaGQu6uOMKL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOwAh_oIOMMU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZLAOq4qOMNx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5BjdRMsOMPk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh03b_VaOMRp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9B5N2X_OMTe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\":0 }#1e-10}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyPulL7tvQOw"
      },
      "outputs": [],
      "source": [
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6yiwXNnvzxO"
      },
      "source": [
        "### Setting up OpenAI GPT-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lzO5PfUpwfv"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI, OpenAIChat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTiEn3tKp7mZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "llm = OpenAIChat(model_name='gpt-3.5-turbo', \n",
        "             temperature=0.9, \n",
        "             max_tokens = 256,\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT8gRDOm_srv"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# openai.ChatCompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCBfxD4cqXsx",
        "outputId": "76d871a6-5c54-428a-f247-4a3554bed49f"
      },
      "outputs": [],
      "source": [
        "text = \"Why did the chicken cross the road?\"\n",
        "\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0AOvbDzfOru"
      },
      "source": [
        "## Cohere "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZXVZwJafLns"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crgiNWjHfOD6"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import Cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU_VG8stfOD7"
      },
      "outputs": [],
      "source": [
        "llm = Cohere(model='command-xlarge-nightly', \n",
        "             temperature=0.9, \n",
        "             max_tokens = 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXZ5uPAafOD7",
        "outputId": "6ce3d6e1-5f07-42c3-d2b1-22045f1f2af1"
      },
      "outputs": [],
      "source": [
        "text = \"Why did the chicken cross the road?\"\n",
        "\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_X9Ds3agedK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_I1CsIKh0Mq"
      },
      "source": [
        "## PromptTemplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFqKU529h5pP"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "template = \"\"\"\n",
        "I want you to act as a naming consultant for new companies.\n",
        "\n",
        "Here are some examples of good company names:\n",
        "\n",
        "- search engine, Google\n",
        "- social media, Facebook\n",
        "- video sharing, YouTube\n",
        "\n",
        "The name should be short, catchy and easy to remember.\n",
        "\n",
        "What is a good name for a company that makes {product}?\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh6-Q9c7iHgD"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "PQphl1RUiffa",
        "outputId": "e683e72e-aadc-4a5a-ddec-a44f7d069530"
      },
      "outputs": [],
      "source": [
        "prompt.format(product=\"colorful socks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B09FUyu6jYAD"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JNnIUgbGjerh",
        "outputId": "3fbaf191-43b2-4746-adc7-06ce0ac9c2ae"
      },
      "outputs": [],
      "source": [
        "response = chain.run(\"Rabbit houses\")\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujgzqqUSkN43"
      },
      "source": [
        "## Jasmine prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-76g4i3ukEdR"
      },
      "outputs": [],
      "source": [
        "template = '''I want you to play the role of Jasmine a programmer at Red Dragon AI. She is 28. She code models in PyTorch. She has a male cat called Pixel. She loves pizza\n",
        "\n",
        "Engage actively in a chat playing the role of Jasmine ans learn as much about the human as possible. Only generate a single response from Jasmine and never from the human.\n",
        "/n/n\n",
        "\n",
        "{human_chat}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxGvxB9OkEfg"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"human_chat\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpm_gwpakEhV"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Np51AKnlkEja",
        "outputId": "c58d47b5-add7-470f-a847-47bbee4b8f73"
      },
      "outputs": [],
      "source": [
        "response = chain.run(\"Tell me about yourself?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNGOl2eYkElQ"
      },
      "outputs": [],
      "source": [
        "def talk_to_Jasmine(text_input):\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"human_chat\"],\n",
        "        template=template,\n",
        "    )\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    response = chain.run(text_input)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "dc5uU7mpm-2U",
        "outputId": "8bd351be-9159-41a0-9b66-0d1df8e7a322"
      },
      "outputs": [],
      "source": [
        "talk_to_Jasmine('Tell me about your cat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltEBHEH6iwU1"
      },
      "outputs": [],
      "source": [
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.llms import OpenAI\n",
        "\n",
        "# llm = OpenAI(temperature=0.9)\n",
        "# prompt = PromptTemplate(\n",
        "#     input_variables=[\"product\"],\n",
        "#     template=\"What is a good name for a company that makes {product}?\",\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
